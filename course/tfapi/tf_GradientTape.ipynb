{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 近似求导(derivative/gradient)\n",
    "\n",
    "#### 方法一：写求导函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    f = 3x^2 + 2x - 1\n",
    "    f' = 6x + 2\n",
    "    \"\"\"\n",
    "    return 3. * x ** 2 + 2. * x - 1\n",
    "\n",
    "\n",
    "def approximate_derivative(f, x, eps=1e-3):\n",
    "    return (f(x + eps) - f(x - eps)) / (2. * eps)\n",
    "\n",
    "\n",
    "print(approximate_derivative(f, 1.))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.999999999993236, 41.999999999994486)\n"
     ]
    }
   ],
   "source": [
    "def g(x1, x2):\n",
    "    \"\"\"\n",
    "    (x1 + 5) * x2^2\n",
    "    \"\"\"\n",
    "    return (x1 + 5) * x2 ** 2\n",
    "\n",
    "\n",
    "def approximate_gradient(g, x1, x2, eps=1e-3):\n",
    "    dg_x1 = approximate_derivative(lambda x: g(x, x2), x1, eps)\n",
    "    dg_x2 = approximate_derivative(lambda x: g(x1, x), x2, eps)\n",
    "    return dg_x1, dg_x2\n",
    "\n",
    "\n",
    "print(approximate_gradient(g, 2., 3.))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 方法二：使用 tf.GradientTape()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=8.999998>, <tf.Tensor: shape=(), dtype=float32, numpy=41.999996>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-08 17:40:05.638354: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-08-08 17:40:05.638832: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(2.)\n",
    "x2 = tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    # func\n",
    "    z = g(x1, x2)\n",
    "\n",
    "dz_x1x2 = tape.gradient(z, [x1, x2])\n",
    "print(dz_x1x2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=8.999998>, <tf.Tensor: shape=(), dtype=float32, numpy=41.999996>]\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant(2.)\n",
    "x2 = tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x1)\n",
    "    tape.watch(x2)\n",
    "    # func\n",
    "    z = g(x1, x2)\n",
    "\n",
    "dz_x1x2 = tape.gradient(z, [x1, x2])\n",
    "print(dz_x1x2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=12.999999>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(5.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = 3 * x\n",
    "    z2 = x ** 2\n",
    "# z1' + z2'\n",
    "tape.gradient([z1, z2], x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 方法二：使用 tf.GradientTape() 求二阶导数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None, <tf.Tensor: shape=(), dtype=float32, numpy=5.9999995>], [<tf.Tensor: shape=(), dtype=float32, numpy=5.9999995>, <tf.Tensor: shape=(), dtype=float32, numpy=14.0>]]\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as outer_tape:\n",
    "    with tf.GradientTape(persistent=True) as inner_tape:\n",
    "        z = g(x1, x2)\n",
    "    inner_grads = inner_tape.gradient(z, [x1, x2])\n",
    "outer_grads = [outer_tape.gradient(inner_grad, [x1, x2])\n",
    "               for inner_grad in inner_grads]\n",
    "\n",
    "print(outer_grads)\n",
    "\n",
    "del inner_tape\n",
    "del outer_tape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 实现梯度下降 (Gradient Descent) 算法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33333334>\n"
     ]
    }
   ],
   "source": [
    "# 写法一\n",
    "\n",
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "\n",
    "# 梯度下降 100 次\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # f = 3x^2 + 2x - 1\n",
    "        z = f(x)\n",
    "        dz_dx = tape.gradient(z, x)\n",
    "        # x = x - lr * grad\n",
    "        x.assign_sub(learning_rate * dz_dx)\n",
    "print(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33333334>\n"
     ]
    }
   ],
   "source": [
    "# 写法二\n",
    "\n",
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "# 梯度下降 100 次\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # f = 3x^2 + 2x - 1\n",
    "        z = f(x)\n",
    "        dz_dx = tape.gradient(z, x)\n",
    "        # x = x - lr * grad\n",
    "        optimizer.apply_gradients([(dz_dx, x)])\n",
    "print(x)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}